{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Image Captioning Project Documentation**\n",
        "# Project Overview\n",
        "This project demonstrates the use of a pre-trained image captioning model to generate descriptive captions for images. The project leverages the BLIP (Bootstrapped Language Image Pre-training) model from Salesforce, which is designed to understand and describe images in natural language.\n",
        "# Objectives\n",
        "Utilize a pre-trained BLIP model for generating captions from images.\n",
        "Implement a streamlined process to handle and generate captions for individual images.\n",
        "Ensure the code is optimized to run on a CPU for ease of use in low-resource environments.\n",
        "# Tools and Libraries\n",
        "Python: Programming language used for the implementation.\n",
        "Transformers: Hugging Face library for accessing pre-trained models.\n",
        "Pillow (PIL): Python Imaging Library for image processing.\n",
        "Torch: PyTorch library for handling tensor operations and model inference.\n"
      ],
      "metadata": {
        "id": "AfpAqqDNGKDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install transformers torch torchvision\n"
      ],
      "metadata": {
        "id": "y_AaT33aDz-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "\n",
        "# Load pre-trained model and processor\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# Path to your image file\n",
        "image_file_path = r\"/content/drive/MyDrive/example.jpg\"\n",
        "\n",
        "# Function to generate captions for an image\n",
        "def generate_captions(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Generate and print caption for the single image\n",
        "caption = generate_captions(image_file_path)\n",
        "print(f\"Image: {image_file_path}\\nCaption: {caption}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "s-6X4A3nFM70",
        "outputId": "f4137ef5-aad6-4867-b9ab-d145dcaae542"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: /content/drive/MyDrive/example.jpg\n",
            "Caption: a dog is walking on the beach with a frumbnt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the below code id for accesing the folder"
      ],
      "metadata": {
        "id": "qp_SXLdpFdeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Load pre-trained model and processor\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# Path to your small dataset\n",
        "image_folder = \"path_to_your_small_image_folder\"\n",
        "# Function to generate captions for images\n",
        "def generate_captions(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Process all images in the folder\n",
        "for image_file in os.listdir(image_folder):\n",
        "    if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        image_path = os.path.join(image_folder, image_file)\n",
        "        caption = generate_captions(image_path)\n",
        "        print(f\"Image: {image_file}\\nCaption: {caption}\\n\")\n"
      ],
      "metadata": {
        "id": "imE8GZFeFbyy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}